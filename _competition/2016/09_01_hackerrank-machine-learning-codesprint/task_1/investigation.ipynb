{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HR competition](https://www.hackerrank.com/contests/machine-learning-codesprint/challenges/hackerrank-predict-email-opens). Evaluation is done using [F1 score](https://en.wikipedia.org/wiki/F1_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from sklearn.metrics import f1_score as f1\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn import preprocessing, cross_validation\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import linear_model, svm, neighbors, ensemble, naive_bayes\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('input/training_dataset.csv')\n",
    "df_2 = pd.read_csv('input/test_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification problem on the unbalanced dataset 0.33%.\n",
    "\n",
    "Only `mail_category`, `last_online`, `hacker_timezone` have missing data. The amount of missing is very small:\n",
    " - mail_category < 0.001\n",
    " - last_online < 0.001\n",
    " - hacker_timezone ~ 1%\n",
    "\n",
    "A couple of fields that are categorical, but presented as strings:\n",
    " - `mail_category`. Training (17 + null), test (14 + null).\n",
    " - `mail_type`. Training test has (4 + null), test has (1 + null). Will remove it.\n",
    " \n",
    "Also there are a few timestamp fields. By themself they are useless. Will extract some data from their deltas. **May be try to extract month/day when email was sent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert(df):\n",
    "    df['mail_category'] = pd.to_numeric(df['mail_category'].str.split('y_').str.get(-1))\n",
    "    \n",
    "    timestamp_fields = ['sent_time', 'last_online', 'hacker_created_at']\n",
    "    for el in timestamp_fields:\n",
    "        df[el] = df[el].astype('datetime64[s]')\n",
    "        \n",
    "    df['age_sent'] = (df['sent_time'] - df['hacker_created_at']) / np.timedelta64(1, 'D')\n",
    "    df['last_seen'] = (df['sent_time'] - df['last_online']) / np.timedelta64(1, 'D')\n",
    "    \n",
    "    df.drop(['mail_type'] + timestamp_fields, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1.drop(['click_time', 'clicked', 'open_time', 'unsubscribe_time', 'unsubscribed'], axis=1, inplace=True)\n",
    "\n",
    "convert(df_1)\n",
    "convert(df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of anonymizing `mail_id` and `user_id` as a long string (most probably `base64(hash)`) is pretty stupid and inefficient. Will convert them to integers to save space. I got **31,440** unique userIds and **164** mail campains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_ids = pd.concat([df_1['user_id'], df_2['user_id']]).unique().tolist()\n",
    "mail_ids = pd.concat([df_1['mail_id'], df_2['mail_id']]).unique().tolist()\n",
    "\n",
    "user_map = {v: k for k, v in enumerate(user_ids)}\n",
    "mail_map = {v: k for k, v in enumerate(mail_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1['user_id'] = df_1['user_id'].apply(lambda x: user_map[x])\n",
    "df_2['user_id'] = df_2['user_id'].apply(lambda x: user_map[x])\n",
    "\n",
    "df_1['mail_id'] = df_1['mail_id'].apply(lambda x: mail_map[x])\n",
    "df_2['mail_id'] = df_2['mail_id'].apply(lambda x: mail_map[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add open rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gb = df_1[['user_id', 'opened']].groupby('user_id')\n",
    "user_info = (gb.sum() / gb.count()).reset_index()\n",
    "user_info.columns = ['user_id', 'opened_rate']\n",
    "\n",
    "df_1 = df_1.merge(user_info, on='user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataframes are only numeric. There are some boolean and some categorical, but no strings. I do not like very long names and the order of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    'contest_login_count'          : 'clc_all',\n",
    "    'contest_login_count_1_days'   : 'clc_1',\n",
    "    'contest_login_count_7_days'   : 'clc_7',\n",
    "    'contest_login_count_30_days'  : 'clc_30',\n",
    "    'contest_login_count_365_days' : 'clc_365',\n",
    "    \n",
    "    'contest_participation_count'           : 'cpc_all',\n",
    "    'contest_participation_count_1_days'    : 'cpc_1',\n",
    "    'contest_participation_count_7_days'    : 'cpc_7',\n",
    "    'contest_participation_count_30_days'   : 'cpc_30',\n",
    "    'contest_participation_count_365_days'  : 'cpc_365',\n",
    "    \n",
    "    'submissions_count'         : 'subm_all',\n",
    "    'submissions_count_1_days'  : 'subm_1',\n",
    "    'submissions_count_7_days'  : 'subm_7',\n",
    "    'submissions_count_30_days' : 'subm_30',\n",
    "    'submissions_count_365_days': 'subm_365',\n",
    "    \n",
    "    'submissions_count_contest'         : 'subm_c_all',\n",
    "    'submissions_count_contest_1_days'  : 'subm_c_1',\n",
    "    'submissions_count_contest_7_days'  : 'subm_c_7',\n",
    "    'submissions_count_contest_30_days' : 'subm_c_30',\n",
    "    'submissions_count_contest_365_days': 'subm_c_365',\n",
    "    \n",
    "    'submissions_count_master'         : 'subm_m_all',\n",
    "    'submissions_count_master_1_days'  : 'subm_m_1',\n",
    "    'submissions_count_master_7_days'  : 'subm_m_7',\n",
    "    'submissions_count_master_30_days' : 'subm_m_30',\n",
    "    'submissions_count_master_365_days': 'subm_m_365',\n",
    "    \n",
    "    'ipn_count'         : 'ipn_all',\n",
    "    'ipn_count_1_days'  : 'ipn_1',\n",
    "    'ipn_count_7_days'  : 'ipn_7',\n",
    "    'ipn_count_30_days' : 'ipn_30',\n",
    "    'ipn_count_365_days': 'ipn_365',\n",
    "    \n",
    "    'ipn_read'         : 'ipnr_all',\n",
    "    'ipn_read_1_days'  : 'ipnr_1',\n",
    "    'ipn_read_7_days'  : 'ipnr_7',\n",
    "    'ipn_read_30_days' : 'ipnr_30',\n",
    "    'ipn_read_365_days': 'ipnr_365',\n",
    "    \n",
    "    'forum_comments_count' : 'forum_reply',\n",
    "    'forum_count'          : 'forum_cnt',\n",
    "    'forum_expert_count'   : 'forum_exp',\n",
    "    'forum_questions_count': 'forum_quest',\n",
    "    'hacker_confirmation'  : 'confirmed',\n",
    "    'hacker_timezone'      : 'timezone',\n",
    "    'mail_category'        : 'mail_cat'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1.rename(columns=rename_dict, inplace=True)\n",
    "df_2.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "order = [\n",
    "    'user_id', 'age_sent', 'last_seen', 'mail_id', 'mail_cat', 'timezone', 'confirmed',\n",
    "    \n",
    "    'forum_reply', 'forum_cnt', 'forum_exp', 'forum_quest',\n",
    "    \n",
    "    'clc_all', 'clc_1', 'clc_7', 'clc_30', 'clc_365',\n",
    "    'cpc_all', 'cpc_1', 'cpc_7', 'cpc_30', 'cpc_365',\n",
    "    'subm_all', 'subm_1', 'subm_7', 'subm_30', 'subm_365',\n",
    "    'subm_c_all', 'subm_c_1', 'subm_c_7', 'subm_c_30', 'subm_c_365',\n",
    "    'subm_m_all', 'subm_m_1', 'subm_m_7', 'subm_m_30', 'subm_m_365',\n",
    "    'ipn_all', 'ipn_1', 'ipn_7', 'ipn_30', 'ipn_365',\n",
    "    'ipnr_all', 'ipnr_1', 'ipnr_7', 'ipnr_30', 'ipnr_365',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1 = df_1[order + ['opened']]\n",
    "df_2 = df_2[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Now almost everything is ready to investigate the dataset. Do not forget to check whether the user_id propagate hidden information. \n",
    "\n",
    "```\n",
    "    user_1 = set(df_1['user_id'].unique())\n",
    "    user_2 = set(df_2['user_id'].unique())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, here is a list of categorical values with their cardinality:\n",
    " - user_id **30538**\n",
    " - mail_id **164**\n",
    " - mail_cat **18**\n",
    " - timezone **23**\n",
    " \n",
    "I will ignore user_id for now, will truncate other categories to exclude everything that is less than 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def truncate_categorical(field, num):\n",
    "    tmp = df_1[field].value_counts()\n",
    "    vals = set(tmp[tmp < num].index.values)\n",
    "\n",
    "    df_1[field][df_1[field].isin(vals)] = -1\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "truncate_categorical('mail_id', 400);\n",
    "truncate_categorical('timezone', 400);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of Null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1['mail_cat'].fillna(df_1['mail_cat'].value_counts().index.values[-1], inplace=True) # the least popular category\n",
    "df_1['timezone'].fillna(-1, inplace=True) # group of least popular timezones\n",
    "df_1['last_seen'].fillna(df_1['last_seen'].mean(), inplace=True) # mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1['mail_cat'] = df_1['mail_cat'].astype(int)\n",
    "df_1['timezone'] = df_1['timezone'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_y = df_1['opened']\n",
    "df_1.drop(['opened'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Trying to do ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyse_many(estimators, X_train, y_train, X_test, y_test):\n",
    "    all_values = []\n",
    "    for estimator, name in estimators:\n",
    "        startTime = datetime.now()\n",
    "        score = f1(y_test, estimator.fit(X_train, y_train).predict(X_test))\n",
    "        time_delta = datetime.now() - startTime\n",
    "        all_values.append((score, name, time_delta.total_seconds()))\n",
    "        \n",
    "        print name, '\\n  ', score, '\\t', time_delta\n",
    "    \n",
    "    clear_output()\n",
    "    all_values.sort(reverse=True)\n",
    "    return pd.DataFrame(all_values, columns=['Score', 'Name', 'Time seconds']).set_index(['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimators_new = []\n",
    "for clf, name in [\n",
    "    (linear_model.LogisticRegression,   'Logistic balanced'),\n",
    "    (linear_model.RidgeClassifier,      'Ridge balanced'),\n",
    "]:\n",
    "    for w in xrange(710, 820, 5):\n",
    "        estimators_new.append((clf(class_weight={1: w, 0: 1000 - w}), name + ' ' + str(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    (linear_model.PassiveAggressiveClassifier(class_weight='balanced'),    'Passive aggressive'),\n",
    "    (linear_model.SGDClassifier(class_weight='balanced'),                  'SGD'),\n",
    " \n",
    "    (ensemble.RandomForestClassifier(class_weight='balanced'),             'Random forest'),\n",
    "    (ensemble.AdaBoostClassifier(),                                        'Ada Boost'),\n",
    "    (ensemble.GradientBoostingClassifier(),                                'Gradient Boosting'),\n",
    "    (ensemble.BaggingClassifier(),                                         'Bagging'),\n",
    "    (ensemble.ExtraTreesClassifier(class_weight='balanced'),               'Extra tree')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standard_flow(df):\n",
    "    df_x = pd.get_dummies(df, columns=['mail_id', 'mail_cat', 'timezone'])\n",
    "\n",
    "    X = df_x.values[:,1:]\n",
    "    Y = df_y.values\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, Y, test_size=0.40, random_state=0)\n",
    "    \n",
    "    return (\n",
    "        analyse_many(estimators_new, X_train, y_train, X_test, y_test),\n",
    "        analyse_many(estimators,     X_train, y_train, X_test, y_test),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_1, score_1_all = standard_flow(df_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD balanced had a reasonable results peaking at `0.507926591882 SGD balanced 807`.\n",
    "\n",
    "Logistic and Ridge performed the best in the region from 700 to 800. Maximum is **0.519678**, achived with **Logistic balanced 744** (in a region from 735 to 745)\n",
    "\n",
    "Tried the same without minMax scaler resulted in **6 times longer** processing and the maximum performance dropped very slightly.\n",
    "\n",
    "\n",
    "#### Second attempt\n",
    "\n",
    "Now I will try to cap some of the series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1[df_1['age_sent'] >= df_1['age_sent'].quantile(0.9)] = df_1['age_sent'].quantile(0.98)\n",
    "data_arr = [\n",
    "    ('last_seen', 400, 500),\n",
    "    ('forum_reply', 80, 100),\n",
    "    ('forum_cnt', 40, 60),\n",
    "    ('forum_exp', 5, 7),\n",
    "    ('forum_quest', 5, 7),\n",
    "    ('clc_all', 40, 60),\n",
    "    ('clc_365', 40, 60),\n",
    "    ('cpc_all', 100, 120),\n",
    "    ('cpc_7', 20, 30),\n",
    "    ('cpc_30', 50, 60),\n",
    "    ('cpc_365', 100, 110),\n",
    "    ('subm_1', 40, 60),\n",
    "    ('subm_c_1', 40, 50),\n",
    "    ('subm_m_1', 40, 50)\n",
    "]\n",
    "for name, max_val, cap_val in data_arr:\n",
    "    df_1[df_1[name] >= max_val] = cap_val\n",
    "\n",
    "def log_transform(x):\n",
    "    return 0 if x == 0 else np.log2(x) + 1\n",
    "    \n",
    "for name in [\n",
    "    'subm_all', 'subm_30', 'subm_365', 'subm_c_all', 'subm_c_7', 'subm_7', 'subm_c_365', 'subm_c_30',\n",
    "    'subm_m_all', 'subm_m_7', 'subm_m_30', 'subm_m_365'\n",
    "]:\n",
    "    df_1[name] = df_1[name].apply(log_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_2, score_2_all = standard_flow(df_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best achieved score is **0.516699** with `Logistic balanced 744`. It is slightly worse than the starting score.\n",
    "\n",
    "-------\n",
    "#### Third attempt\n",
    "\n",
    "Now trying to get rid of `ipnr_*` `ipn_*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1_copy = df_1.copy()\n",
    "\n",
    "df_1_copy['ipn_all_percent'] = (df_1_copy['ipnr_all'] / df_1_copy['ipn_all']).fillna(0)\n",
    "df_1_copy['ipn_1_percent']   = (df_1_copy['ipnr_1']   / df_1_copy['ipn_1']).fillna(0)\n",
    "df_1_copy['ipn_7_percent']   = (df_1_copy['ipnr_7']   / df_1_copy['ipn_7']).fillna(0)\n",
    "df_1_copy['ipn_30_percent']  = (df_1_copy['ipnr_30']  / df_1_copy['ipn_30']).fillna(0)\n",
    "df_1_copy['ipn_365_percent'] = (df_1_copy['ipnr_365'] / df_1_copy['ipn_365']).fillna(0)\n",
    "\n",
    "df_1_copy.drop(['ipnr_all', 'ipn_all', 'ipnr_1', 'ipn_1', 'ipnr_7', 'ipn_7', 'ipnr_30', 'ipn_30', 'ipnr_365', 'ipn_365'], axis=1, inplace=True)\n",
    "    \n",
    "score_3, score_3_all = standard_flow(df_1_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forth attempt\n",
    "\n",
    "To use `fillna(0.5)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1['ipn_all_percent'] = (df_1['ipnr_all'] / df_1['ipn_all']).fillna(0.5)\n",
    "df_1['ipn_1_percent']   = (df_1['ipnr_1']   / df_1['ipn_1']).fillna(0.5)\n",
    "df_1['ipn_7_percent']   = (df_1['ipnr_7']   / df_1['ipn_7']).fillna(0.5)\n",
    "df_1['ipn_30_percent']  = (df_1['ipnr_30']  / df_1['ipn_30']).fillna(0.5)\n",
    "df_1['ipn_365_percent'] = (df_1['ipnr_365'] / df_1['ipn_365']).fillna(0.5)\n",
    "\n",
    "df_1.drop(['ipnr_all', 'ipn_all', 'ipnr_1', 'ipn_1', 'ipnr_7', 'ipn_7', 'ipnr_30', 'ipn_30', 'ipnr_365', 'ipn_365'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_4, score_4_all = standard_flow(df_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fifth attempt\n",
    "\n",
    "remove a lot of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1.drop([\n",
    "    'clc_all', 'clc_30', 'clc_365',\n",
    "    'cpc_all', 'cpc_30', 'cpc_365',\n",
    "    'subm_all', 'subm_30', 'subm_365',\n",
    "    'subm_c_all', 'subm_c_30', 'subm_c_365',\n",
    "    'subm_m_all', 'subm_m_30', 'subm_m_365'\n",
    "], axis=1, inplace=True)\n",
    "    \n",
    "score_5, score_5_all = standard_flow(df_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For truncate_categorical (100, 300):\n",
    " - all values **0.519454**\n",
    " - capping series **0.518768**\n",
    " - got rid of `ipnr_*` `ipn_*`, filling with 0 **0.516237**\n",
    " - got rid of `ipnr_*` `ipn_*`, filling with 0.5 **0.518523**\n",
    " - remove many columns **0.516990**\n",
    " \n",
    "For truncate categorical (400, 300). Number uniques (106, 22):\n",
    " - all values **0.519506**\n",
    " - capping series **0.518668**\n",
    " - got rid of `ipnr_*` `ipn_*`, filling with 0 **0.516095**\n",
    " - got rid of `ipnr_*` `ipn_*`, filling with 0.5 **0.518402**\n",
    " - remove many columns **0.516817**\n",
    " \n",
    "For truncate categorical (600, 600). Number uniques (86, 21):\n",
    " - all values **0.519513**\n",
    " - capping series **0.518545**\n",
    " - got rid of `ipnr_*` `ipn_*`, filling with 0 **0.516087**\n",
    " - got rid of `ipnr_*` `ipn_*`, filling with 0.5 **0.518440**\n",
    " - remove many columns **0.516732**\n",
    " \n",
    "Best PCA reduction technique achieved **0.513982**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Time seconds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.520189</th>\n",
       "      <td>Logistic balanced 730</td>\n",
       "      <td>29.789122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.519671</th>\n",
       "      <td>Logistic balanced 745</td>\n",
       "      <td>70.282320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.517143</th>\n",
       "      <td>Logistic balanced 750</td>\n",
       "      <td>49.974155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.519915</th>\n",
       "      <td>Logistic balanced 745</td>\n",
       "      <td>43.636953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.518200</th>\n",
       "      <td>Logistic balanced 740</td>\n",
       "      <td>28.724288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Name  Time seconds\n",
       "Score                                        \n",
       "0.520189  Logistic balanced 730     29.789122\n",
       "0.519671  Logistic balanced 745     70.282320\n",
       "0.517143  Logistic balanced 750     49.974155\n",
       "0.519915  Logistic balanced 745     43.636953\n",
       "0.518200  Logistic balanced 740     28.724288"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([\n",
    "    score_1.head(1),\n",
    "    score_2.head(1),\n",
    "    score_3.head(1),\n",
    "    score_4.head(1),\n",
    "    score_5.head(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
